# Global configuration
global:
  target_model: "meta-llama/Llama-3.2-3B"
  datasets:
    - mimir_name: "arxiv"
      split: "ngram_13_0.8"
      # split: "ngram_7_0.2"
    # - mimir_name: "pile_cc"
    #   split: "ngram_13_0.8"
    # - mimir_name: "wikipedia_en"
    #   split: "ngram_13_0.8"
    #   split: "ngram_7_0.2"
    # - mimir_name: "dm_mathematics"
    #   split: "ngram_13_0.8"
    #   split: "ngram_7_0.2"
    # - mimir_name: "github"
    #   split: "ngram_13_0.8"
    #   split: "ngram_7_0.2"
    # - mimir_name: "hackernews"
    #   split: "ngram_13_0.8"
    #   split: "ngram_7_0.2"
    # - mimir_name: "pubmed_central"
    #   split: "ngram_13_0.8"
    #   split: "ngram_7_0.2"
  batch_size: 16  # Increased per GPU batch size
  seed: 42
  device: "cuda"  # This will use all available GPUs
  fpr_thresholds:
    - 0.1
    - 0.01
  n_bootstrap_samples: 10
  test_samples: null  # Adjust this number as needed
  num_epochs: 30  # Increased for overfitting


loss:
  module: loss

zlib:
  module: zlib

lowercase:
  module: lowercase
  batch_size: 8

mink:
  module: mink
  k: 20
  batch_size: 2

minkplusplus:
  module: minkplusplus
  k: 20
  batch_size: 2

recall:
  module: recall
  extra_non_member_dataset: "imperial-cpg/copyright-traps-extra-non-members"
  split: "seq_len_100"
  batch_size: 2
  n_shots: 7
  match_perplexity: false
  fixed_prefix: true

conrecall:
  module: conrecall
  extra_non_member_dataset: "imperial-cpg/copyright-traps-extra-non-members"
  split: "seq_len_100"
  batch_size: 2
  n_shots: 7
  match_perplexity: false

ratio:
  module: ratio
  reference_model_path: "openlm-research/open_llama_7b"
  batch_size: 1
  device: "cuda"

bag_of_words:
  module: bag_of_words
  test_size: 0.2
  min_df: 0.05
  n_estimators: 100
  max_depth: 2
  min_samples_leaf: 5
  seed: 42

ensemble_classifier:
  module: ensemble_classifier
  batch_size: 1
  n_shots: 7
  reference_model_path: "openlm-research/open_llama_7b"
  extra_non_member_dataset: "imperial-cpg/copyright-traps-extra-non-members"
  split: "seq_len_100"
  # MinK thresholds for feature extraction
  k_thresholds: [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
